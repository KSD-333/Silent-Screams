# Silent Screams - ML-based Monitoring System

A local, privacy-first ML system for detecting abnormal behavioral patterns through pose estimation and sequence classification. The system supports both live camera monitoring and uploaded video analysis.

## üéØ Features

- **Live Camera Monitoring**: Real-time pose tracking with instant alerts
- **Video Upload Analysis**: Batch processing of recorded videos with timeline view
- **Privacy-First**: 100% local processing, no cloud connectivity required
- **Customizable Alerts**: Configurable thresholds, sounds, and detection parameters
- **Cross-Platform**: Works on Windows, macOS, and Linux

## üöÄ Quick Start

### 1. Installation

```bash
# Clone or download the project
cd silent-screams

# Create a virtual environment (recommended)
python -m venv venv

# Activate virtual environment
# Windows:
venv\Scripts\activate
# macOS/Linux:
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt
```

### 2. Run the Application

```bash
streamlit run app.py
```

The GUI will open in your default browser at `http://localhost:8501`

## üìÅ Project Structure

```
silent-screams/
‚îú‚îÄ‚îÄ app.py                    # Streamlit GUI application
‚îú‚îÄ‚îÄ keypoint_extractor.py     # MediaPipe pose extraction
‚îú‚îÄ‚îÄ models.py                 # ML model architectures (LSTM, Transformer)
‚îú‚îÄ‚îÄ inference.py              # Real-time monitoring and video processing
‚îú‚îÄ‚îÄ dataset_utils.py          # Data preprocessing utilities
‚îú‚îÄ‚îÄ train_lstm.py             # Training script stub
‚îú‚îÄ‚îÄ requirements.txt          # Python dependencies
‚îú‚îÄ‚îÄ alarm.wav                 # Default alert sound (optional)
‚îú‚îÄ‚îÄ models/                   # Trained model files (.h5 or SavedModel)
‚îÇ   ‚îî‚îÄ‚îÄ mock_model.h5         # Auto-generated demo model
‚îî‚îÄ‚îÄ README.md                 # This file
```

## üéÆ Usage

### Live Monitoring Mode

1. Click **"Start Live Monitoring"** in the main interface
2. Grant camera permissions when prompted
3. The system will:
   - Display live video with skeleton overlay
   - Show predicted class and probability in real-time
   - Trigger visual + audio alerts when abnormal behavior detected
   - Log all alerts with timestamps

4. Adjust settings in the left sidebar:
   - **Model Path**: Path to trained model file
   - **Threshold**: Detection sensitivity (0.0-1.0)
   - **Window Length**: Number of frames to analyze (default: 30)
   - **Frame Rate**: Processing FPS (default: 15)
   - **Sound File**: Custom alert sound path
   - **Mute Audio**: Disable alert sounds

### Video Upload Mode

1. Click **"Analyze Uploaded Video"**
2. Upload a video file (MP4, AVI, MOV, etc.)
3. The system will:
   - Process the entire video
   - Display progress bar
   - Generate a timeline of detected events
   - Show clickable timestamps with thumbnails

4. Click any detection to:
   - View the frame thumbnail
   - See exact timestamp (HH:MM:SS)
   - Review confidence score

## ü§ñ Using a Trained Model

### Option 1: Use Demo Mode (No Model Required)

The system automatically generates a mock model for testing. This is useful for:
- Testing the GUI and workflow
- Developing custom training pipelines
- Demonstrating the system without labeled data

### Option 2: Train Your Own Model

1. **Collect Training Data**:
   ```python
   # Use keypoint_extractor.py to extract features from videos
   from keypoint_extractor import extract_keypoints_from_video
   
   features = extract_keypoints_from_video("video.mp4")
   # Save features with labels for training
   ```

2. **Prepare Dataset**:
   - Organize sequences into normal/abnormal classes
   - Save as `.npy` files or TFRecords
   - Format: `(num_sequences, window_length, num_features)`

3. **Train Model**:
   ```bash
   python train_lstm.py --data_path ./training_data --epochs 50
   ```

4. **Use Trained Model**:
   - Place model file in `models/` directory
   - Update "Model Path" in GUI settings
   - Start monitoring

### Model Input Format

Models expect input shape: `(batch_size, window_length, num_features)`
- **window_length**: Number of frames (default: 30)
- **num_features**: Flattened pose keypoints (default: 66 for 33 landmarks √ó 2 coords)

Output: Binary classification (0=normal, 1=abnormal) with probability

## üîß Configuration

### Settings (Sidebar)

| Setting | Default | Description |
|---------|---------|-------------|
| Model Path | `models/mock_model.h5` | Path to trained Keras model |
| Threshold | 0.8 | Minimum probability for alert (0.0-1.0) |
| Window Length | 30 | Frames per sequence (e.g., 2s @ 15fps) |
| Frame Rate | 15 | Processing FPS (lower = faster) |
| Sound File | `alarm.wav` | Custom alert sound path |
| Mute Audio | False | Disable alert sounds |

### Advanced Configuration

Edit these constants in `inference.py`:
- `COOLDOWN_SECONDS`: Time between alerts (default: 1.5s)
- `MERGE_THRESHOLD`: Merge detections within N seconds (default: 2.0s)
- `INFERENCE_STRIDE`: Run inference every N frames (default: 5)

## üéì Training Your Own Model

### Data Collection

1. Record videos of normal and abnormal behaviors
2. Extract keypoints:
   ```python
   from keypoint_extractor import KeypointExtractor
   
   extractor = KeypointExtractor()
   features, confidences = extractor.process_video("video.mp4")
   ```

3. Create sliding windows:
   ```python
   from dataset_utils import create_sliding_windows
   
   windows = create_sliding_windows(features, window_length=30, stride=15)
   ```

4. Label sequences: 0 (normal) or 1 (abnormal)

### Training Script

See `train_lstm.py` for a complete training pipeline:

```bash
# Basic training
python train_lstm.py --data_path ./data --epochs 50

# Advanced options
python train_lstm.py \
    --data_path ./data \
    --model_type lstm \
    --window_length 30 \
    --epochs 100 \
    --batch_size 32 \
    --learning_rate 0.001
```

### Model Architecture Options

1. **Bi-LSTM** (Default): Good for temporal patterns
   - 2 Bi-LSTM layers (128 units each)
   - Dropout regularization
   - Best for: Sequential behavior patterns

2. **Transformer**: Better for long-range dependencies
   - Multi-head self-attention
   - Positional encoding
   - Best for: Complex temporal relationships

Edit `models.py` to customize architectures.

## üîí Privacy & Ethics

### Privacy Considerations

- **Local Processing**: All computation happens on your device
- **No Data Storage**: Video frames are processed in memory only
- **No Network Calls**: System works completely offline
- **No Logging**: Personal data is never saved to disk

### Ethical Use

This system is designed for:
- ‚úÖ Personal safety monitoring with consent
- ‚úÖ Research and development
- ‚úÖ Accessibility applications
- ‚úÖ Educational purposes

**NOT intended for**:
- ‚ùå Surveillance without consent
- ‚ùå Discriminatory profiling
- ‚ùå Privacy violations
- ‚ùå Unauthorized monitoring

**Important**: Always obtain informed consent before monitoring individuals. Comply with local privacy laws and regulations.

## üêõ Troubleshooting

### Camera Not Working

- **Windows**: Grant camera permissions in Settings ‚Üí Privacy ‚Üí Camera
- **macOS**: System Preferences ‚Üí Security & Privacy ‚Üí Camera
- **Linux**: Check `/dev/video0` permissions

### Low FPS / Performance Issues

- Reduce "Frame Rate" in settings (try 10 or 5 FPS)
- Reduce "Window Length" (try 20 frames)
- Close other applications using camera
- Use GPU acceleration (install `tensorflow-gpu`)

### Model Loading Errors

- Ensure model file exists at specified path
- Check model was trained with compatible TensorFlow version
- Verify input shape matches: `(window_length, num_features)`
- Try demo mode with mock model first

### Audio Not Playing

- Check system volume and audio output device
- Verify `alarm.wav` exists or system supports programmatic beep
- Try toggling "Mute Audio" off
- On Linux, install: `sudo apt-get install python3-pyaudio`

### MediaPipe Detection Issues

- Ensure good lighting conditions
- Subject should be fully visible in frame
- Avoid cluttered backgrounds
- Check camera resolution (720p or higher recommended)

## üî¨ Technical Details

### Pose Estimation

- **Library**: MediaPipe Pose
- **Landmarks**: 33 body keypoints (shoulders, elbows, wrists, hips, knees, ankles, etc.)
- **Normalization**: Coordinates normalized by torso length
- **Missing Data**: Fill-forward or zero-padding with confidence masking

### Sequence Classification

- **Window Length**: 30 frames (2 seconds @ 15fps)
- **Stride**: 5 frames for inference (overlap for smoothness)
- **Features**: 66 dimensions (33 landmarks √ó 2 coords)
- **Optional**: Add velocity features (delta x, y)

### Alert Logic

1. Maintain rolling buffer of last T frames
2. Run inference every N frames
3. If probability > threshold: trigger alert
4. Cooldown period prevents alert spam
5. Post-process: merge close detections in video mode

## üìä Performance

Typical performance on modern hardware:

| Hardware | Live FPS | Video Processing |
|----------|----------|------------------|
| CPU (Intel i5) | 10-15 fps | 0.5x realtime |
| CPU (Intel i7) | 15-20 fps | 1.0x realtime |
| GPU (NVIDIA GTX 1060) | 25-30 fps | 3.0x realtime |
| GPU (NVIDIA RTX 3070) | 30+ fps | 5.0x realtime |

## ü§ù Contributing

This is a local-first, privacy-focused project. Contributions welcome:

1. Improve model architectures
2. Add new feature extraction methods
3. Enhance GUI/UX
4. Optimize performance
5. Add unit tests

## üìÑ License

This project is provided for educational and research purposes. Use responsibly and ethically.

## üôè Acknowledgments

- **MediaPipe**: Google's pose estimation framework
- **TensorFlow**: ML framework
- **Streamlit**: GUI framework
- **OpenCV**: Computer vision library

---

**Built with privacy and ethics in mind. Use responsibly.**
